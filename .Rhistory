library(SparkR)
install.packages("SparkR")
install.packages("SparkR")
library(SparkR)
sparkR.version
if (!require('devtools')) install.packages('devtools')
devtools::install_github('apache/spark@v1.6.0', subdir='R/pkg')
load(SparkR)
library(SparkR)
args <- commandArgs(trailing = TRUE)
print(args)
print(args)
if (length(args) != 1) {
print("Usage: data-manipulation.R <path-to-flights.csv")
print("The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv ")
q("no")
}
library(SparkR)
args <- commandArgs(trailing = TRUE)
print(args)
if (length(args) != 1) {
print("Usage: data-manipulation.R <path-to-flights.csv")
print("The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv ")
q("no")
}
sc <- sparkR.init(appName = "SparkR-data-manipulation-example")
library(SparkR)
args <- commandArgs(trailing = TRUE)
sc <- sparkR.init(appName = "SparkR-data-manipulation-example")
SPARK_HOME = "/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6/bin"
sc <- sparkR.init(appName = "SparkR-data-manipulation-example")
sys.setenv(SPARK_HOME = "/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6/bin")
sys.setenv(SPARK_HOME = "/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6")
sc <- sparkR.init(appName = "SparkR-data-manipulation-example")
Sys.getenv("SPARK_HOME")
sc <- sparkR.init(master="/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6/", appName = "SparkR-data-manipulation-example")
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6")
}
Sys.getenv("SPARK_HOME")
if (!require('devtools')) install.packages('devtools')
devtools::install_github('apache/spark@v1.6.0', subdir='R/pkg')
library(SparkR)
args <- commandArgs(trailing = TRUE)
print(args)
if (length(args) != 1) {
print("Usage: data-manipulation.R <path-to-flights.csv")
print("The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv ")
}
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
if (!require('devtools')) install.packages('devtools')
devtools::install_github('apache/spark@v1.6.0', subdir='R/pkg')
library(SparkR)
## SPEC THE INITIAL STUFF
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6")
}
Sys.getenv("SPARK_HOME")
Sys.getenv("SPARK_HOME")
## SPEC THE INITIAL STUFF
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6")
}
Sys.getenv("SPARK_HOME")
sc <- sparkR.init(master="/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6/", appName = "SparkR-data-manipulation-example")
sqlContext <- sparkRSQL.init(sc)
sc<- sparkR.init(master="/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6/", appName = "SparkR-data-manipulation-example")
sqlContext <- sparkRSQL.init(sc)
help(sparkR)
??sparkR
sc<- sparkR.init(master="/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6/R/lib/sparkr.zip", appName = "SparkR-data-manipulation-example")
sqlContext <- sparkRSQL.init(sc)
sc<- sparkR.init(master="/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6/R/lib/", appName = "SparkR-data-manipulation-example")
library(SparkR)
library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
rlib<-library(SparkR, lib.loc = c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
print(rlib)
rlib
lib.loc <- c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"))
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
sqlContext <- sparkRSQL.init(sc)
rlib<-library(c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib")))
lib.loc
sqlContext <- sparkRSQL.init(sc)
library(readr)
flights <- read_csv("~/Documents/GitHub/sparkR/sparkR/flights.csv")
View(flights)
flights_df <- read_csv("~/Documents/GitHub/sparkR/sparkR/flights.csv", header = TRUE)
flights_df <- read_csv("~/Documents/GitHub/sparkR/sparkR/flights.csv")
flights_df$date <- as.Date(flights_df$date)
SFO_df <- flights_df[flights_df$dest == "SFO", ]
SFO_DF <- createDataFrame(sqlContext, SFO_df)
SFO_DF <- createDataFrame(sqlContext, SFO_df)
# Convert the local data frame into a SparkR DataFrame
SFO_DF <- createDataFrame(sqlContext, SFO_df)
flightsDF <- read.df(sqlContext, "~/Documents/GitHub/sparkR/sparkR/flights.csv", source = "com.databricks.spark.csv", header = "true")
read.df??
flightsDF <- read.df(sqlContext, "~/Documents/GitHub/sparkR/sparkR/flights.csv")
flightsDF <- read.df(sqlContext, "~/Documents/GitHub/sparkR/sparkR/flights.csv", "csv", header = "true")
printSchema(flightsDF)
flightsDF <- read.df(sqlContext, "~/Documents/GitHub/sparkR/sparkR/flights.csv", "csv", header = "true")
flightsDF <- read.df(sqlContext, "~/Documents/GitHub/sparkR/sparkR/flights.csv", source = "csv", header = "true")
flightsDF <- read.df("~/Documents/GitHub/sparkR/sparkR/flights.csv", source = "com.databricks.spark.csv", header = "true")
flightsDF <- read.df("~/Documents/GitHub/sparkR/sparkR/flights.csv", source = "csv", header = "true")
flightsDF <- read.df(sqlContext, "~/Documents/GitHub/sparkR/sparkR/flights.csv", source = "csv", header = "true")
import_iris <- copy_to(sc, iris, "spark_iris", overwrite = TRUE)
library(SparkR)
## SPEC THE INITIAL STUFF
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6")
}
Sys.getenv("SPARK_HOME")
## Initialize SparkContext
lib.loc <- c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"))
lib.loc
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
import_iris <- copy_to(sc, iris, "spark_iris", overwrite = TRUE)
copy_to
library(SparkR)
## SPEC THE INITIAL STUFF
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6")
}
Sys.getenv("SPARK_HOME")
## Initialize SparkContext
lib.loc <- c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"))
lib.loc
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
import_iris <- copy_to(sc, iris, "spark_iris", overwrite = TRUE)
library(SparkR)
install.packages("sparklyr")
## SPEC THE INITIAL STUFF
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6")
}
Sys.getenv("SPARK_HOME")
## Initialize SparkContext
lib.loc <- c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"))
lib.loc
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
install.packages("sparklyr")
import_iris <- copy_to(sc, iris, "spark_iris", overwrite = TRUE)
library(SparkR)
install.packages("sparklyr")
## SPEC THE INITIAL STUFF
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6")
}
Sys.getenv("SPARK_HOME")
## Initialize SparkContext
lib.loc <- c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"))
lib.loc
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
import_iris <- copy_to(sc, iris, "spark_iris", overwrite = TRUE)
library(dplyr)
library(dplyr)
## SPEC THE INITIAL STUFF
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6")
}
Sys.getenv("SPARK_HOME")
## Initialize SparkContext
lib.loc <- c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"))
lib.loc
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
import_iris <- copy_to(sc, iris, "spark_iris", overwrite = TRUE)
import_iris <- copy_to(sc, iris, "spark_iris", overwrite = TRUE)
install.packages(c("nycflights13"))
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
library(nycflights13)
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
sc <- sparkR.init(master = "local[*]", sparkEnvir = list(spark.driver.memory="2g"))
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
iris2 <- dbplyr::src_memdb() %>% copy_to(iris, overwrite = TRUE)
library(dplyr)
devtools::install_github("rstudio/sparklyr")
