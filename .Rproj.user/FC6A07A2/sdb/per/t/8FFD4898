{
    "collab_server" : "",
    "contents" : "#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# For this example, we shall use the \"flights\" dataset\n# The dataset consists of every flight departing Houston in 2011.\n# The data set is made up of 227,496 rows x 14 columns. \n\n# To run this example use\n# ./bin/sparkR --packages com.databricks:spark-csv_2.10:1.0.3\n#     examples/src/main/r/data-manipulation.R <path_to_csv>\n\n# Load SparkR library into your R session\n\nif (!require('devtools')) install.packages('devtools')\ndevtools::install_github('apache/spark@v1.6.0', subdir='R/pkg')\nlibrary(SparkR)\n\nargs <- commandArgs(trailing = TRUE)\nprint(args)\nif (length(args) != 1) {\n  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n}\n\n## SPEC THE INITIAL STUFF\nif (nchar(Sys.getenv(\"SPARK_HOME\")) < 1) {\n  Sys.setenv(SPARK_HOME = \"/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6\")\n}\nSys.getenv(\"SPARK_HOME\")\n\n## Initialize SparkContext\nlib.loc <- c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\"))\nlib.loc\nsc <- sparkR.init(master = \"local[*]\", sparkEnvir = list(spark.driver.memory=\"2g\"))\n\n##sc<- sparkR.init(master=\"/Users/Susana/Documents/spark-1.6.0-bin-hadoop2.6/R/lib/\", appName = \"SparkR-data-manipulation-example\")\n\n## Initialize SQLContext\nsqlContext <- sparkRSQL.init(sc)\n\nflights <- read_csv(\"~/Documents/GitHub/sparkR/sparkR/flights.csv\")\n\n# Create a local R dataframe\nflights_df <- read_csv(\"~/Documents/GitHub/sparkR/sparkR/flights.csv\")\nflights_df$date <- as.Date(flights_df$date)\n\n## Filter flights whose destination is San Francisco and write to a local data frame\nSFO_df <- flights_df[flights_df$dest == \"SFO\", ] \n\n# Convert the local data frame into a SparkR DataFrame\nSFO_DF <- createDataFrame(sqlContext, SFO_df)\n\n#  Directly create a SparkR DataFrame from the source data\nflightsDF <- read.df(\"~/Documents/GitHub/sparkR/sparkR/flights.csv\", \"csv\", header = \"true\")\nread.df??\nflightsDF <- read.df(sqlContext, \"~/Documents/GitHub/sparkR/sparkR/flights.csv\", source = \"csv\", header = \"true\")\n\n# Print the schema of this Spark DataFrame\nprintSchema(flightsDF)\n\n# Cache the DataFrame\ncache(flightsDF)\n\n# Print the first 6 rows of the DataFrame\nshowDF(flightsDF, numRows = 6) ## Or\nhead(flightsDF)\n\n# Show the column names in the DataFrame\ncolumns(flightsDF)\n\n# Show the number of rows in the DataFrame\ncount(flightsDF)\n\n# Select specific columns\ndestDF <- select(flightsDF, \"dest\", \"cancelled\")\n\n# Using SQL to select columns of data\n# First, register the flights DataFrame as a table\nregisterTempTable(flightsDF, \"flightsTable\")\ndestDF <- sql(sqlContext, \"SELECT dest, cancelled FROM flightsTable\")\n\n# Use collect to create a local R data frame\nlocal_df <- collect(destDF)\n\n# Print the newly created local data frame\nhead(local_df)\n\n# Filter flights whose destination is JFK\njfkDF <- filter(flightsDF, \"dest = \\\"JFK\\\"\") ##OR\njfkDF <- filter(flightsDF, flightsDF$dest == \"JFK\")\n\n# If the magrittr library is available, we can use it to\n# chain data frame operations\nif(\"magrittr\" %in% rownames(installed.packages())) {\n  library(magrittr)\n\n  # Group the flights by date and then find the average daily delay\n  # Write the result into a DataFrame\n  groupBy(flightsDF, flightsDF$date) %>%\n    summarize(avg(flightsDF$dep_delay), avg(flightsDF$arr_delay)) -> dailyDelayDF\n\n  # Print the computed data frame\n  head(dailyDelayDF)\n}\n\n# Stop the SparkContext now\nsparkR.stop()\n",
    "created" : 1517449382346.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "837040131",
    "id" : "8FFD4898",
    "lastKnownWriteTime" : 1518647896,
    "last_content_update" : 1518647896677,
    "path" : "~/Documents/spark-1.6.0-bin-hadoop2.6/examples/src/main/r/data-manipulation.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}